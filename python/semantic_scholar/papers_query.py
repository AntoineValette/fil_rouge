import requests
import time
import json  # Pour sauvegarder les rÃ©sultats

from python.semantic_scholar.print_article import printArticle


def fetch_all_responses(base_url):
    """
    RÃ©cupÃ¨re toutes les pages de rÃ©ponses (en suivant le token de pagination)
    et retourne une liste contenant l'ensemble des rÃ©ponses.
    
    Logique ajoutÃ©e :
      - Lorsqu'un token dÃ©jÃ  vu est rencontrÃ©, on stocke la rÃ©ponse dans r_seen.
      - Au dÃ©but de la boucle interne, on compare le token courant au token de la boucle prÃ©cÃ©dente.
        Si les deux sont identiques, on prend le token contenu dans r_seen.
        Sinon, on prend le token courant (r['token']).
    """
    responses = []
    headers = {"User-Agent": "SemanticScholarScraper/1.0"}
    seen_tokens = set()
    
    # Pour mÃ©moriser le token de la boucle prÃ©cÃ©dente et la rÃ©ponse associÃ©e en cas de doublon
    prev_token = None
    r_seen = None

    while True:
        try:
            response = requests.get(base_url, headers=headers)
            
            if response.status_code == 429:  # Trop de requÃªtes
                wait_time = 10
                print(f"âš ï¸ Trop de requÃªtes ! Attente de {wait_time}s avant de rÃ©essayer...")
                time.sleep(wait_time)
                continue  # RÃ©essayer immÃ©diatement aprÃ¨s la pause
            
            response.raise_for_status()  # VÃ©rifie les erreurs HTTP
            r = response.json()
            responses.append(r)
            
            print(f"ğŸ“Š Nombre total annoncÃ© par l'API : {r.get('total', 'inconnu')}")

            # Boucle de pagination
            while "token" in r:
                # VÃ©rification en dÃ©but de boucle :
                # Si le token courant est identique au token de la boucle prÃ©cÃ©dente
                # et qu'une rÃ©ponse duplicate a Ã©tÃ© stockÃ©e, on utilise son token.
                if prev_token is not None and r.get("token") == prev_token and r_seen is not None:
                    token = r_seen.get("token")
                    print(f"ğŸ”„ Token inchangÃ©, utilisation du token stockÃ© dans r_seen : {token}")
                else:
                    token = r.get("token")

                if token is None:
                    print("âœ… Fin de pagination : plus de token disponible.")
                    break  # Fin de la pagination

                # Si le token a dÃ©jÃ  Ã©tÃ© vu, on stocke la rÃ©ponse dans r_seen
                # puis on continue pour forcer la rÃ©cupÃ©ration d'une nouvelle page.
                if token in seen_tokens:
                    print(f"ğŸ” Token {token} dÃ©jÃ  rÃ©cupÃ©rÃ©, stockage dans r_seen et passage Ã  la page suivante...")
                    r_seen = r
                    time.sleep(3)  # Petite pause pour Ã©viter le throttling
                    response = requests.get(f"{base_url}&token={token}", headers=headers)
                    if response.status_code == 429:
                        wait_time = 10
                        print(f"âš ï¸ Trop de requÃªtes ! Attente de {wait_time}s avant de rÃ©essayer...")
                        time.sleep(wait_time)
                        continue  # RÃ©essayer aprÃ¨s la pause
                    response.raise_for_status()
                    r_seen = response.json()
                    continue  # Retour en dÃ©but de boucle pour vÃ©rifier le token actualisÃ©
                
                # Nouveau token non vu, on l'ajoute Ã  l'ensemble
                seen_tokens.add(token)
                print(f"ğŸ“„ Passage Ã  la page suivante avec token : {token}") 

                time.sleep(3)  # Pause pour Ã©viter le throttling
                response = requests.get(f"{base_url}&token={token}", headers=headers)
                if response.status_code == 429:
                    wait_time = 10
                    print(f"âš ï¸ Trop de requÃªtes ! Attente de {wait_time}s avant de rÃ©essayer...")
                    time.sleep(wait_time)
                    continue
                response.raise_for_status()
                r = response.json()
                if "token" not in r:
                    print("âš ï¸ L'API ne fournit plus de token aprÃ¨s cette page.")
                
                responses.append(r)
                # MÃ©morisation du token courant pour la prochaine itÃ©ration
                prev_token = token
                # On rÃ©initialise r_seen puisque le token a changÃ©
                r_seen = None

            return responses  # Sortie propre de la boucle principale

        except requests.exceptions.RequestException as e:
            print(f"âŒ Erreur rÃ©seau : {e}")
            wait_time = 10
            print(f"ğŸ•’ Attente de {wait_time}s avant de rÃ©essayer...")
            time.sleep(wait_time)  # Pause avant de rÃ©essayer

    return responses



def combine_data(responses):
    """
    Combine les listes "data" de chacune des rÃ©ponses en une seule liste.
    """
    combined = []
    for resp in responses:
        if "data" in resp:
            combined.extend(resp["data"])
    return combined


def get_data_JSON(year=2020):
    """
    RÃ©cupÃ¨re les donnÃ©es des articles publiÃ©s entre 2020 et 2023 avec l'API Semantic Scholar.
    """

    # DÃ©finition des paramÃ¨tres de la requÃªte
    query = "artificial intelligence | AI"
    fields = ("paperId,corpusId,externalIds,url,title,authors,venue,publicationVenue,"
              "year,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,journal,abstract")

    all_data = []

    for month in range(1, 13):  # De janvier (1) Ã  dÃ©cembre (12)
        date_filter = f"{year}-{month:02d}"  # Format YYYY-MM

        url = (f"http://api.semanticscholar.org/graph/v1/paper/search/bulk?query={query}"
                f"&publicationDateOrYear={date_filter}"
                f"&fields={fields}"
                f"&maxResults=10000")  # RÃ©cupÃ©ration par blocs de 10 000

        print(f"ğŸ” RequÃªte envoyÃ©e pour {date_filter}")

        responses = fetch_all_responses(url)
        all_data.extend(combine_data(responses))  # Fusionner les rÃ©sultats

        print(f"ğŸ“Š Articles rÃ©cupÃ©rÃ©s jusquâ€™ici : {len(all_data)}")
        
        # ğŸ’¡ Pause de 3 secondes pour Ã©viter dâ€™Ãªtre bloquÃ© par lâ€™API
        time.sleep(3)

    print(f"âœ… Nombre total d'articles rÃ©cupÃ©rÃ©s en {year} : {len(all_data)}")

    return all_data


if __name__ == "__main__":
    articles = get_data_JSON(year=2020)
    for article in articles[:5]:  # Afficher les 5 premiers articles
        print(article)
        printArticle(article)
